# How to Create a Benchmark

To create a benchmark, you need to write a function that returns a `BenchmarkResult` instance.
This object is dictionary-like and holds information about the model results on the benchmark. 

For example if you submitted an
EfficientNet model to an ImageNet benchmark, the instance would contain information on its performance (Top 1/5 Accuracy), the model name, the name
of the dataset and task, and so on. The object also contains methods for serialising the results to JSON, and some server checking methods that call the sotabench.com API to check if the results can be accepted.

If you want to see the full API for `BenchmarkResult`, then skip to the end of this section. 
Otherwise we will go through a step-by-step example in PyTorch for creating a benchmarl.

## The Bare Necessities

Start a new project and make a benchmark file, e.g. `mnist.py`. Begin by writing a skeleton function
as follows:

```python
from sotabenchapi.core import BenchmarkResult

def evaluate_mnist(...) -> BenchmarkResult:
    
    # your evaluation logic here
    results = {...} # dict with keys as metric names, values as metric results
    
    return BenchmarkResult(results=results)
```

This is the core structure of an evaluation method for sotabench: we have a function that takes in user inputs, 
we do some evaluation, and we pass in some results and other outputs to a `BenchmarkResult` instance. Essentially you can write any benchmark around this format,
and take in any input that you want for your evaluation. It is designed to be flexible.

For example, it could be as simple as taking a json of predictions as an input if that's all you need. Or if you 
want more information about the model, you could request a model function or class as an input and pass the data to the 
model yourself. It is up to you how you want to design your benchmark.

## Sotabench Metadata

So that benchmark results can be displayed on sotabench.com, you will need your submissions to have metadata about the model name, 
the dataset name and the task. For example, "EfficientNet", "Imagenet", "Image Classification".

In the context of your benchmark function:

```python
from sotabenchapi.core import BenchmarkResult

DATASET_NAME = 'ImageNet'
TASK = 'Image Classification'

def evaluate_mnist(model_name, ...) -> BenchmarkResult:
    
    # your evaluation logic here
    results = {...} # dict with keys as metric names, values as metric results
    
    return BenchmarkResult(results=results, model=model_name, dataset=DATASET_NAME, task=TASK)
```

Here the dataset name and task name will be fixed for the benchmark, but the model name
can be specified as an input. You can add additional metadata to connect things like the 
ArXiv paper id - see the API documentation at the end of this section for more information.

## Example: An MNIST benchmark in PyTorch

Let's see how we might make a PyTorch friendly benchmark which adheres to the framework's abstractions.

The first thing we need for evaluation is a dataset! Let's use the MNIST dataset from the `torchvision` library, 
along with a `DataLoader`:

```python
from sotabenchapi.core import BenchmarkResult
from torch.utils.data import DataLoader
import torchvision.datasets as datasets

def evaluate_mnist(data_root: str, batch_size: int = 32, num_workers: int = 4) -> BenchmarkResult:
    
    dataset = datasets.MNIST(data_root, train=False, download=True)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return BenchmarkResult(dataset=dataset.__name__)
```

We've set `train=false` since we want to use the testing split for evaluation. We've also added a `data_root` parameter
just so the use can specify where they want the data downloaded.

We should also probably allow for the user to put in their own transforms since this is a vision dataset, so 
let's modify further:

```python
from sotabenchapi.core import BenchmarkResult
from torch.utils.data import DataLoader
import torchvision.datasets as datasets

def evaluate_mnist(data_root: str, batch_size: int = 32, num_workers: int = 4, 
                   input_transform=None, target_transform=None) -> BenchmarkResult:
    
    dataset = datasets.MNIST(data_root, transform=input_transform, target_transform=target_transform, 
        train=False, download=True)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return BenchmarkResult(dataset=dataset.__name__)
```

Great, so we have a dataset set up. Let's now take in a model. We could do this in a number of ways, for example,
we could accept a model function as an input (that takes in data and outputs predictions). Since we are using PyTorch,
where most modules are subclasses of `nn.Module`, let's do it in an object-oriented way by accepting a model object input:

```python
from sotabenchapi.core import BenchmarkResult
import torchvision.datasets as datasets
from torchbench.utils import send_model_to_device

def evaluate_mnist(model, data_root: str, batch_size: int = 32, num_workers: int = 4, 
                   input_transform=None, target_transform=None) -> BenchmarkResult:
    
    model, device = send_model_to_device(model, device='cuda')
    model.eval()
    
    dataset = datasets.MNIST(data_root, transform=input_transform, target_transform=target_transform, 
        train=False, download=True)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return BenchmarkResult(dataset=dataset.__name__)
```

Here we have reused a function from `torchbench` for sending the model to a cuda device, but this is optional - you can
decide how models are processed in your own benchmark however you see fit.

Now that we have a model and a dataset, let's loop through and evaluate the model:

```python
from sotabenchapi.core import BenchmarkResult
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
from torchbench.utils import send_model_to_device, default_data_to_device, AverageMeter, accuracy
import tqdm
import torch

def evaluate_mnist(model, data_root: str, batch_size: int = 32, num_workers: int = 4, 
                   input_transform=None, target_transform=None) -> BenchmarkResult:
    
    model, device = send_model_to_device(model, device='cuda')
    model.eval()
    
    dataset = datasets.MNIST(data_root, transform=input_transform, target_transform=target_transform, 
        train=False, download=True)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    top1 = AverageMeter()
    top5 = AverageMeter()

    with torch.no_grad():
        for i, (input, target) in enumerate(tqdm.tqdm(loader)):

            input, target = default_data_to_device(input, target, device=device)
            output = model(input)
            prec1, prec5 = accuracy(output, target, topk=(1, 5))
            top1.update(prec1.item(), input.size(0))
            top5.update(prec5.item(), input.size(0))

    results = {'Top 1 Accuracy': top1.avg, 'Top 5 Accuracy': top5.avg}

    return BenchmarkResult(dataset=dataset.__name__, results=results)
```

We've used some more utility functions from `torchbench`, but again, you can use whatever you want to do evaluation.
You can see we've passed a results dictionary into the `BenchmarkResult` object. Great! So we have a function that
takes in a model and evaluates on a dataset. But how do we connect to Sotabench? Well, we need to have the user pass
in some metadata information about the model name and paper id. We also need to specify a bit more about our benchmark,
e.g. the task in this case is "Image Classification":

```python
from sotabenchapi.core import BenchmarkResult
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
from torchbench.utils import send_model_to_device, default_data_to_device, AverageMeter, accuracy
import tqdm
import torch

def evaluate_mnist(model, data_root: str, batch_size: int = 32, num_workers: int = 4, 
                   input_transform=None, target_transform=None, model_name:str = None,
                   arxiv_id:str = None) -> BenchmarkResult:
    
    model, device = send_model_to_device(model, device='cuda')
    model.eval()
    
    dataset = datasets.MNIST(data_root, transform=input_transform, target_transform=target_transform, 
        train=False, download=True)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    top1 = AverageMeter()
    top5 = AverageMeter()

    with torch.no_grad():
        for i, (input, target) in enumerate(tqdm.tqdm(loader)):

            input, target = default_data_to_device(input, target, device=device)
            output = model(input)
            prec1, prec5 = accuracy(output, target, topk=(1, 5))
            top1.update(prec1.item(), input.size(0))
            top5.update(prec5.item(), input.size(0))

    results = {'Top 1 Accuracy': top1.avg, 'Top 5 Accuracy': top5.avg}

    return BenchmarkResult(task='Image Classification', dataset=dataset.__name__, results=results, 
                           model=model_name, arxiv_id=arxiv_id)
```

And you're set! The task string connects to the taxonomy on sotabench, the rest gives context to the
result - for example the model's name and the paper it is from.

The final step is to publish this as a PyPi library. This will enable your users to write a `sotabench.py` file
that imports your benchmark and passes their model and other parameters into it. When they connect to sotabench.com,
sotabench.com will download your library and evaluate their model with it, and then publish the results to your
benchmark page.

## Other Examples

The [torchbench](https://www.github.com/paperswithcode/torchbench) library is a good reference for benchmark implementations, 
which you can base your own benchmarks on.

## API for BenchmarkResult

```eval_rst

.. automodule:: sotabenchapi.core.results
   :members:
```
